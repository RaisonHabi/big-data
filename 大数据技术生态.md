## 大数据技术本质上解决4个核心问题
1.存储，海量的数据怎样有效的存储？主要包括hdfs、Kafka；  
2.计算，海量的数据怎样快速计算？主要包括MapReduce、Spark、Flink等；  
3.查询，海量数据怎样快速查询？主要为Nosql和OLAP。Nosql主要包括Hbase、Cassandra等，OLAP包括kylin、impla等。Nosql主要解决随机查询，OLAP技术主要解决关联查询；  
4.挖掘，海量数据怎样挖掘出隐藏的知识？也就是当前火热的机器学习和深度学习等技术，包括TensorFlow、caffe、mahout等；



hive底层计算：MapReduce/Spark

hadoop是基于文件的数据结构;  
spark是基于RDD的数据结构，计算性能要比hadoop高


&nbsp;
## hadoop / hive / hbase 的区别
### **hadoop**  
一个分布式计算+分布式文件系统，前者其实就是 MapReduce，后者是 HDFS 。  
后者可以独立运行，前者可以选择性使用，也可以不使用  
### **hive**  
通俗的说是一个数据仓库，仓库中的数据是被hdfs管理的数据文件，它支持类似sql语句的功能，你可以通过该语句完成分布式环境下的计算功能，hive会把语句转换成MapReduce，然后交给hadoop执行。  
这里的计算，仅限于查找和分析，而不是更新、增加和删除。  
它的优势是对历史数据进行处理，用时下流行的说法是离线计算，因为它的底层是MapReduce，MapReduce在实时计算上性能很差。  
它的做法是把数据文件加载进来作为一个hive表（或者外部表），让你觉得你的sql操作的是传统的表。  
### **hbase**  
Hadoop database 的简称，通俗的说，hbase的作用类似于数据库，传统数据库管理的是集中的本地数据文件，而hbase基于hdfs实现对分布式数据文件的管理，比如增删改查。  
也就是说，**hbase只是利用hadoop的hdfs帮助其管理数据的持久化文件（HFile），它跟MapReduce没任何关系**。  
hbase的优势在于实时计算，所有实时数据都直接存入hbase中，客户端通过API直接访问hbase，实现实时计算。  
由于它使用的是nosql，或者说是列式结构，从而提高了查找性能，使其能运用于大数据场景，这是它跟MapReduce的区别。
### 总结
**hadoop是hive和hbase的基础，hive依赖hadoop，而hbase仅依赖hadoop的hdfs模块。**  
**hive适用于离线数据的分析**，操作的是通用格式的（如通用的日志文件）、被hadoop管理的数据文件，它支持类sql，比编写MapReduce的java代码来的更加方便，它的***定位是数据仓库，存储和分析历史数据***。  
**hbase适用于实时计算，采用列式结构的nosql**，操作的是自己生成的特殊格式的HFile、被hadoop管理的数据文件，它的***定位是数据库，或者叫DBMS***。  
hive可以直接操作hdfs中的文件作为它的表的数据，也可以使用hbase数据库作为它的表。

&nbsp;
## hadoop、hbase、hive、spark分布式系统架构原理
hadoop用户分布式存储和map-reduce计算，spark用于分布式机器学习，hive是分布式数据库，hbase是分布式kv系统，看似互不相关的他们却都是基于相同的hdfs存储和yarn（类似于npm的包管理工具，由facebook推出并开源）资源管理。 

```
Hbase：是一个nosql数据库，和mongodb类似 

hdfs：hadoop distribut file system，hadoop的分布式文件系统 

Hive：hive是基于Hadoop的一个数据仓库工具，将结构化的数据文件（或者非结构化的数据）映射为一张数据库表，
并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 

sqoop：sqoop是和Hive一起使用的。Sqoop(发音：skup)是一款开源的工具，
主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql…)间进行数据的传递。 
```
hadoop分为几大部分：yarn负责资源和任务管理、hdfs负责分布式存储、map-reduce负责分布式计算 

### MR
MapReduce编程模型： 一种分布式计算模型框架，解决海量数据的计算问题 

MapReduce将整个并行计算过程抽象到两个函数 
```
Map（映射）：对一些独立元素组成的列表的每一个元素进行指定的操作，可以高度并行。 
Reduce（化简）：对一个列表的元素进行合并。 
```

### spark
Spark的主要特性是它的**内存中集群计算，提高了应用程序的处理速度**。 

spark的核心是RDD（Resilient Distributed Datasets弹性分布式数据集），一种通用的数据抽象，封装了基础的数据操作，如map，filter，reduce等。 

RDD提供数据共享的抽象，相比其他大数据处理框架，如MapReduce，Pegel，DryadLINQ和HIVE等均缺乏此特性，所以RDD更为通用。 

简要地概括RDD：RDD是一个不可修改的，分布的对象集合。每个RDD由多个分区组成，每个分区可以同时在集群中的不同节点上计算。RDD可以包含Python，Java和Scala中的任意对象。 

### pyspark
为了用Spark支持Python，Apache Spark社区发布了一个工具PySpark。使用PySpark，您也可以使用Python编程语言处理RDD。 
### SparkContext
SparkContext是任何spark功能的入口点。当我们运行任何Spark应用程序时，会启动一个驱动程序，它具有main函数，并且此处启动了SparkContext。   
1）.SparkContext示例 - PySpark Shell    
2）. SparkContext示例 - Python程序    
然后在终端中执行以下命令来运行此Python文件：spark-submit demo.py 


### spark实例： 
https://blog.csdn.net/gongpulin/article/details/51534754 
```
a. 案例描述 
b．案例分析 
c. 编程实现：SparkWordCount 类源码 
d. 提交到集群执行 
./spark-submit \ 
--class com.ibm.spark.exercise.basic.SparkWordCount \ 
--master spark://hadoop036166:7077 \ 
--num-executors 3 \ 
--driver-memory 6g --executor-memory 2g \ 
--executor-cores 2 \ 
/home/fams/sparkexercise.jar \ 
hdfs://hadoop036166:9000/user/fams/*.txt 

e. 监控执行状态 
该实例把最终的结果存储在了 HDFS 上，那么如果程序运行正常我们可以在 HDFS 上找到生成的文件信息 
```

&nbsp;
## reference  
1.[如何用形象的比喻描述大数据的技术生态？](https://www.zhihu.com/question/27974418)
